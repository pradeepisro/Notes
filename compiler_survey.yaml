- Loop Rotation:
  authors: Michael Wolfe(Oregon Graduate Center, Department of Computer Science and Engineering)
  tldr: This paper describes a new transformation technique for loops. This maps a specific kind of loop called systolic loop on to a Distributed Memory Message Passing multiprocessor(DMMP)
  link: https://dl.acm.org/citation.cfm?id=102862
  tags: loop optimization
- Loop Skewing:
  authors: Michael Wolfe(Department of Computer Science, UIUC)
  tldr: Loop skewing is a transformation technique to derive wavefront method of execution of nested loops. The wavefront method is used to execute nested loop in parallel and vector architectures.
  link: https://link.springer.com/article/10.1007%2FBF01407876
  tags: loop optimization
- The Parallel Execution of Do Loops:
  authors: Leslie Lamport(MIT)
  tldr: Do loops are more frequently used in fortran programmers. The author proposes methods for automatically paralleling iteration of Do loop on an asynchronous multiprocessor and array processor with some restrictions on what statement can be present inside(IO and not transfer of control outside the loop). 
  link: https://www.microsoft.com/en-us/research/publication/parallel-execution-loops/
  tags: loop optimization
- A Code Generator for High-Performance Tensor Contractions on GPUs:
  authors: Jinsung Kim(The Ohio State University), Aravind Sukumaran-Rajam(The Ohio State University), Vineeth Thumma(The Ohio State University), Sriram Krishnamoorthy(Pacific Northwest National Laboratory, Washington),Ajay Panyala(Pacific Northwest National Laboratory, Washington), Louis-Noel Pouchet(Colorado State University), Atanas Rountev(The Ohio State University), P. Sadayappan(The Ohio State University)
  tldr: Tensor Contractions are generalization of MM. They are compute intensive workloads. The paper describes a codegeneration method for tensor contractions targetting GPU(CUDA). It exploits domain-specific properties about data reuse in tensor contractions to devise an effective code generation schema, coupled with an effective model-driven search, to determine parameters for mapping of computation to threads and staging of data through the GPU memory hierarchy
  link: https://dl.acm.org/citation.cfm?id=3314885
  tags: GPU, CGO, Tensor, CUDA
- Automatic translation of FORTRAN Programs to Vector Form:
  authors: Randy Allen(Rice University), Ken Kennedy(Rice University)
  tldr: FORTRAN is widely used in Scientific Computing and "Recent" introduction of HPC systems like Cray-1 and other vector machines has increased the interests of standards commitee to provide it to the language. With Fortran 8.x there will be new vector operations but inorder to port existing applications to F8x we need to quite some refactoring. This can be avoided by using automatic translation, but it is not very trivial at first to convert F7x Do loops to F8x vector operations because they are semantically different in the sense that there is a problem of dependence between Do loops iterations. The idea is to use data dependence to uncover parallelism(even the hidden ones).
  link: https://dl.acm.org/citation.cfm?id=29875
  tags: automatic parallelization, vector, fortran
- Spectres, Virtual Ghosts, and Hardware Support:
  authors: Xiaowan Dong(University of Rochester), Zhuojia Shen(University of Rochester), John Criswell(University of Rochester), Alan Cox(Rice University), Sandhya Dwarkadas(University of Rochester)
  tldir: Side-channel attacks, such as Spectre and Meltdown, that leverage speculative execution pose a serious threat to computing systems.This work evaluates the performance impact of three different defenses against in-kernel speculation side-channel attacks within the context of Virtual Ghost. 
  link: https://dl.acm.org/citation.cfm?id=3214297
  tags: security, spectre, meltdown
- Anatomy of High Performance Matrix Multiplication
  authors: Kazushige goto, Robert A. Van De Geijn(Univeristy of Texas, Austin)
  tldir: Basic principles that underly the implemetation of matrix multiplication in GotoBLAS. It is optimized for multi-level memory hierarchies and various architectures. Bulk computations are done in the form of GEPP, GEMP, GEPM.
  link: https://dl.acm.org/citation.cfm?id=1356053
  tags: Matrix Mulitplication, Optimization
